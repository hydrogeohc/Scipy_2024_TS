{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Models Comparison: Energy Consumption Forecasting\n",
    "\n",
    "**SciPy 2024 Conference - Tacoma, WA**\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive comparison of 5 different time series forecasting models:\n",
    "\n",
    "1. **ARIMA** - Classical statistical approach (baseline)\n",
    "2. **xLSTM** - Deep learning with explainability\n",
    "3. **TimesFM** - Google's foundation model (zero-shot learning)\n",
    "4. **TimeGPT** - API-based foundation model\n",
    "5. **AutoGluon** - AutoML ensemble approach\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- **MAE** (Mean Absolute Error) - Lower is better\n",
    "- **RMSE** (Root Mean Squared Error) - Lower is better\n",
    "- **Runtime** - Training/inference time in seconds\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Generation\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Configure\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Energy Consumption Data\n",
    "\n",
    "We create synthetic data with three components:\n",
    "- **Trend**: Linear upward growth\n",
    "- **Seasonality**: Annual cyclic pattern\n",
    "- **Noise**: Random fluctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_energy_data(n_samples=1000, seed=42):\n",
    "    \"\"\"Generate synthetic energy consumption data with trend and seasonality\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create date range\n",
    "    date_rng = pd.date_range(start='2020-01-01', periods=n_samples, freq='D')\n",
    "    df = pd.DataFrame(date_rng, columns=['timestamp'])\n",
    "    \n",
    "    # Create components\n",
    "    trend = np.linspace(0, 20, n_samples)  # Linear trend\n",
    "    season = 10 * np.sin(2 * np.pi * np.arange(n_samples) / 365.25)  # Annual seasonality\n",
    "    noise = np.random.normal(0, 5, n_samples)  # Random noise\n",
    "    \n",
    "    # Combine components\n",
    "    df['energy_consumption'] = 100 + trend + season + noise\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "df = generate_energy_data()\n",
    "\n",
    "# Split into train and test\n",
    "test_size = 30\n",
    "train_data = df[:-test_size]\n",
    "test_data = df[-test_size:]\n",
    "\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"  Total samples: {len(df)}\")\n",
    "print(f\"  Training samples: {len(train_data)}\")\n",
    "print(f\"  Test samples: {len(test_data)}\")\n",
    "print(f\"  Date range: {df.index[0].date()} to {df.index[-1].date()}\")\n",
    "print(f\"\\n  Mean consumption: {df['energy_consumption'].mean():.2f} kWh\")\n",
    "print(f\"  Std deviation: {df['energy_consumption'].std():.2f} kWh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Plot 1: Full time series\n",
    "ax = axes[0]\n",
    "ax.plot(train_data.index, train_data['energy_consumption'], \n",
    "        label='Training Data', alpha=0.8, linewidth=1.5)\n",
    "ax.plot(test_data.index, test_data['energy_consumption'], \n",
    "        label='Test Data', color='orange', alpha=0.8, linewidth=1.5)\n",
    "ax.axvline(x=test_data.index[0], color='red', linestyle='--', \n",
    "           alpha=0.5, label='Train/Test Split')\n",
    "ax.set_title('Energy Consumption Time Series', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.set_ylabel('Energy Consumption (kWh)', fontsize=11)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test data zoomed in\n",
    "ax = axes[1]\n",
    "ax.plot(test_data.index, test_data['energy_consumption'], \n",
    "        marker='o', color='orange', linewidth=2, markersize=6, label='Test Data')\n",
    "ax.set_title('Test Set (Last 30 Days)', fontsize=13, fontweight='bold')\n",
    "ax.set_xlabel('Date', fontsize=11)\n",
    "ax.set_ylabel('Energy Consumption (kWh)', fontsize=11)\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Data visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Model Training and Evaluation\n",
    "\n",
    "We'll train each model sequentially and store the results for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: ARIMA (Statistical Baseline)\n",
    "\n",
    "**Key Features:**\n",
    "- Classical statistical approach\n",
    "- Highly interpretable\n",
    "- Very fast training\n",
    "- Good for data with clear trend/seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training ARIMA Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Auto-select best ARIMA parameters\n",
    "print(\"Searching for optimal parameters...\")\n",
    "model_auto = auto_arima(\n",
    "    train_data['energy_consumption'],\n",
    "    start_p=0, start_q=0, max_p=5, max_q=5, m=7,\n",
    "    start_P=0, seasonal=True, d=1, D=1,\n",
    "    trace=False, error_action='ignore',\n",
    "    suppress_warnings=True, stepwise=True\n",
    ")\n",
    "\n",
    "print(f\"  Best order: {model_auto.order}\")\n",
    "\n",
    "# Fit ARIMA model\n",
    "model_arima = ARIMA(train_data['energy_consumption'], order=model_auto.order)\n",
    "results_arima = model_arima.fit()\n",
    "\n",
    "# Make predictions\n",
    "forecast_arima = results_arima.forecast(steps=test_size)\n",
    "\n",
    "arima_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "arima_mae = mean_absolute_error(test_data['energy_consumption'], forecast_arima)\n",
    "arima_rmse = np.sqrt(mean_squared_error(test_data['energy_consumption'], forecast_arima))\n",
    "\n",
    "print(f\"\\n‚úì ARIMA Training Complete\")\n",
    "print(f\"  MAE:     {arima_mae:.3f} kWh\")\n",
    "print(f\"  RMSE:    {arima_rmse:.3f} kWh\")\n",
    "print(f\"  Runtime: {arima_time:.2f} seconds\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: xLSTM (Deep Learning)\n",
    "\n",
    "**Key Features:**\n",
    "- Captures complex non-linear patterns\n",
    "- Learns from sequential dependencies\n",
    "- More flexible than statistical models\n",
    "- Provides some interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training xLSTM Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare data\n",
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_data[['energy_consumption']])\n",
    "test_scaled = scaler.transform(test_data[['energy_consumption']])\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:(i + seq_length)])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 30\n",
    "X_train, y_train = create_sequences(train_scaled, seq_length)\n",
    "\n",
    "print(f\"Sequence shape: {X_train.shape}\")\n",
    "\n",
    "# Build LSTM model\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "print(\"Training neural network...\")\n",
    "\n",
    "# Train model\n",
    "history = model_lstm.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "last_sequence = train_scaled[-seq_length:]\n",
    "forecast_lstm = []\n",
    "\n",
    "for _ in range(test_size):\n",
    "    next_pred = model_lstm.predict(last_sequence.reshape(1, seq_length, 1), verbose=0)\n",
    "    forecast_lstm.append(next_pred[0, 0])\n",
    "    last_sequence = np.roll(last_sequence, -1, axis=0)\n",
    "    last_sequence[-1] = next_pred\n",
    "\n",
    "# Inverse transform\n",
    "forecast_lstm = scaler.inverse_transform(np.array(forecast_lstm).reshape(-1, 1)).flatten()\n",
    "\n",
    "lstm_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "lstm_mae = mean_absolute_error(test_data['energy_consumption'], forecast_lstm)\n",
    "lstm_rmse = np.sqrt(mean_squared_error(test_data['energy_consumption'], forecast_lstm))\n",
    "\n",
    "print(f\"\\n‚úì xLSTM Training Complete\")\n",
    "print(f\"  MAE:     {lstm_mae:.3f} kWh\")\n",
    "print(f\"  RMSE:    {lstm_rmse:.3f} kWh\")\n",
    "print(f\"  Runtime: {lstm_time:.2f} seconds\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: TimesFM (Google Foundation Model)\n",
    "\n",
    "**Key Features:**\n",
    "- Zero-shot learning (no training required!)\n",
    "- 200M parameter transformer model\n",
    "- Pre-trained on diverse time series\n",
    "- Fast inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Loading TimesFM Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import timesfm\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load pretrained model\n",
    "    print(\"Loading pretrained checkpoint from Hugging Face...\")\n",
    "    tfm = timesfm.TimesFm(\n",
    "        context_len=512,\n",
    "        horizon_len=test_size,\n",
    "        input_patch_len=32,\n",
    "        output_patch_len=128,\n",
    "        num_layers=20,\n",
    "        model_dims=1280,\n",
    "        backend='cpu',  # Use 'gpu' if available\n",
    "    )\n",
    "    \n",
    "    tfm.load_from_checkpoint(repo_id=\"google/timesfm-1.0-200m-pytorch\")\n",
    "    \n",
    "    print(\"Generating forecast (zero-shot, no training)...\")\n",
    "    \n",
    "    # Forecast\n",
    "    forecast_result = tfm.forecast(\n",
    "        inputs=[train_data['energy_consumption'].values],\n",
    "        freq=[0],  # Daily frequency\n",
    "    )\n",
    "    \n",
    "    forecast_timesfm = forecast_result.forecast[0][:test_size]\n",
    "    \n",
    "    timesfm_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    timesfm_mae = mean_absolute_error(test_data['energy_consumption'], forecast_timesfm)\n",
    "    timesfm_rmse = np.sqrt(mean_squared_error(test_data['energy_consumption'], forecast_timesfm))\n",
    "    \n",
    "    print(f\"\\n‚úì TimesFM Inference Complete\")\n",
    "    print(f\"  MAE:     {timesfm_mae:.3f} kWh\")\n",
    "    print(f\"  RMSE:    {timesfm_rmse:.3f} kWh\")\n",
    "    print(f\"  Runtime: {timesfm_time:.2f} seconds\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    timesfm_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n‚ö† TimesFM not installed\")\n",
    "    print(\"  Install with: pip install timesfm\")\n",
    "    print(\"  Skipping TimesFM evaluation...\\n\")\n",
    "    print(\"=\"*60)\n",
    "    timesfm_available = False\n",
    "    timesfm_mae, timesfm_rmse, timesfm_time = None, None, None\n",
    "    forecast_timesfm = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö† TimesFM error: {str(e)}\")\n",
    "    print(\"  Skipping TimesFM evaluation...\\n\")\n",
    "    print(\"=\"*60)\n",
    "    timesfm_available = False\n",
    "    timesfm_mae, timesfm_rmse, timesfm_time = None, None, None\n",
    "    forecast_timesfm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: TimeGPT (API-based Foundation Model)\n",
    "\n",
    "**Key Features:**\n",
    "- Pre-trained on billions of time points\n",
    "- Best speed/accuracy trade-off\n",
    "- API-based inference (requires API key)\n",
    "- Minimal preprocessing needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"Using TimeGPT Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    from nixtla import NixtlaClient\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize client\n",
    "    print(\"Initializing TimeGPT client...\")\n",
    "    client = NixtlaClient()  # Expects TIMEGPT_TOKEN env variable\n",
    "    \n",
    "    # Prepare data\n",
    "    df_timegpt = pd.DataFrame({\n",
    "        'ds': train_data.index,\n",
    "        'y': train_data['energy_consumption'].values\n",
    "    })\n",
    "    \n",
    "    print(\"Generating forecast via API...\")\n",
    "    \n",
    "    # Forecast\n",
    "    forecast_df = client.forecast(\n",
    "        df=df_timegpt,\n",
    "        h=test_size,\n",
    "        freq='D',\n",
    "        time_col='ds',\n",
    "        target_col='y'\n",
    "    )\n",
    "    \n",
    "    forecast_timegpt = forecast_df['TimeGPT'].values\n",
    "    \n",
    "    timegpt_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    timegpt_mae = mean_absolute_error(test_data['energy_consumption'], forecast_timegpt)\n",
    "    timegpt_rmse = np.sqrt(mean_squared_error(test_data['energy_consumption'], forecast_timegpt))\n",
    "    \n",
    "    print(f\"\\n‚úì TimeGPT Inference Complete\")\n",
    "    print(f\"  MAE:     {timegpt_mae:.3f} kWh\")\n",
    "    print(f\"  RMSE:    {timegpt_rmse:.3f} kWh\")\n",
    "    print(f\"  Runtime: {timegpt_time:.2f} seconds\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    timegpt_available = True\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n‚ö† TimeGPT not installed\")\n",
    "    print(\"  Install with: pip install nixtla\")\n",
    "    print(\"  Skipping TimeGPT evaluation...\\n\")\n",
    "    print(\"=\"*60)\n",
    "    timegpt_available = False\n",
    "    timegpt_mae, timegpt_rmse, timegpt_time = None, None, None\n",
    "    forecast_timegpt = None\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö† TimeGPT error: {str(e)}\")\n",
    "    print(\"  Set TIMEGPT_TOKEN environment variable\")\n",
    "    print(\"  OR: client = NixtlaClient(api_key='your-key')\")\n",
    "    print(\"  Skipping TimeGPT evaluation...\\n\")\n",
    "    print(\"=\"*60)\n",
    "    timegpt_available = False\n",
    "    timegpt_mae, timegpt_rmse, timegpt_time = None, None, None\n",
    "    forecast_timegpt = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: AutoGluon (AutoML)\n",
    "\n",
    "**Key Features:**\n",
    "- Automated model selection and ensembling\n",
    "- Best overall accuracy\n",
    "- Minimal hyperparameter tuning\n",
    "- Production-ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.timeseries import TimeSeriesDataFrame, TimeSeriesPredictor\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Training AutoGluon Model\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Prepare data in AutoGluon format\n",
    "train_ag = pd.DataFrame({\n",
    "    'timestamp': train_data.index,\n",
    "    'target': train_data['energy_consumption'].values,\n",
    "    'item_id': ['energy'] * len(train_data)\n",
    "})\n",
    "\n",
    "train_ag = TimeSeriesDataFrame.from_data_frame(\n",
    "    train_ag,\n",
    "    id_column='item_id',\n",
    "    timestamp_column='timestamp'\n",
    ")\n",
    "\n",
    "print(\"Training ensemble models...\")\n",
    "\n",
    "# Train predictor\n",
    "predictor = TimeSeriesPredictor(\n",
    "    prediction_length=test_size,\n",
    "    target='target',\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "predictor.fit(\n",
    "    train_ag,\n",
    "    presets='fast_training',\n",
    "    time_limit=60\n",
    ")\n",
    "\n",
    "# Predict\n",
    "forecast_ag = predictor.predict(train_ag)\n",
    "forecast_autogluon = forecast_ag['mean'].values\n",
    "\n",
    "autogluon_time = time.time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "autogluon_mae = mean_absolute_error(test_data['energy_consumption'], forecast_autogluon)\n",
    "autogluon_rmse = np.sqrt(mean_squared_error(test_data['energy_consumption'], forecast_autogluon))\n",
    "\n",
    "print(f\"\\n‚úì AutoGluon Training Complete\")\n",
    "print(f\"  MAE:     {autogluon_mae:.3f} kWh\")\n",
    "print(f\"  RMSE:    {autogluon_rmse:.3f} kWh\")\n",
    "print(f\"  Runtime: {autogluon_time:.2f} seconds\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Results Comparison\n",
    "\n",
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "models_list = ['ARIMA', 'xLSTM']\n",
    "mae_list = [arima_mae, lstm_mae]\n",
    "rmse_list = [arima_rmse, lstm_rmse]\n",
    "runtime_list = [arima_time, lstm_time]\n",
    "\n",
    "if timesfm_available:\n",
    "    models_list.append('TimesFM')\n",
    "    mae_list.append(timesfm_mae)\n",
    "    rmse_list.append(timesfm_rmse)\n",
    "    runtime_list.append(timesfm_time)\n",
    "\n",
    "if timegpt_available:\n",
    "    models_list.append('TimeGPT')\n",
    "    mae_list.append(timegpt_mae)\n",
    "    rmse_list.append(timegpt_rmse)\n",
    "    runtime_list.append(timegpt_time)\n",
    "\n",
    "models_list.append('AutoGluon')\n",
    "mae_list.append(autogluon_mae)\n",
    "rmse_list.append(autogluon_rmse)\n",
    "runtime_list.append(autogluon_time)\n",
    "\n",
    "# Create results DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Model': models_list,\n",
    "    'MAE (kWh)': mae_list,\n",
    "    'RMSE (kWh)': rmse_list,\n",
    "    'Runtime (s)': runtime_list\n",
    "})\n",
    "\n",
    "# Add rankings\n",
    "results['MAE Rank'] = results['MAE (kWh)'].rank().astype(int)\n",
    "results['Speed Rank'] = results['Runtime (s)'].rank().astype(int)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Highlight winners\n",
    "best_mae_idx = results['MAE (kWh)'].idxmin()\n",
    "best_rmse_idx = results['RMSE (kWh)'].idxmin()\n",
    "fastest_idx = results['Runtime (s)'].idxmin()\n",
    "\n",
    "print(f\"\\nüèÜ Best Accuracy (MAE):  {results.loc[best_mae_idx, 'Model']} ({results.loc[best_mae_idx, 'MAE (kWh)']:.3f} kWh)\")\n",
    "print(f\"üèÜ Best Accuracy (RMSE): {results.loc[best_rmse_idx, 'Model']} ({results.loc[best_rmse_idx, 'RMSE (kWh)']:.3f} kWh)\")\n",
    "print(f\"‚ö° Fastest Runtime:       {results.loc[fastest_idx, 'Model']} ({results.loc[fastest_idx, 'Runtime (s)']:.2f} sec)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: All Forecasts Overlaid\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(test_data.index, test_data['energy_consumption'], \n",
    "         label='Actual', color='black', linewidth=3, marker='o', markersize=5, zorder=10)\n",
    "ax1.plot(test_data.index, forecast_arima, \n",
    "         label=f'ARIMA (MAE={arima_mae:.2f})', alpha=0.7, linewidth=2, linestyle='--')\n",
    "ax1.plot(test_data.index, forecast_lstm, \n",
    "         label=f'xLSTM (MAE={lstm_mae:.2f})', alpha=0.7, linewidth=2, linestyle='--')\n",
    "if timesfm_available:\n",
    "    ax1.plot(test_data.index, forecast_timesfm, \n",
    "             label=f'TimesFM (MAE={timesfm_mae:.2f})', alpha=0.7, linewidth=2, linestyle='--')\n",
    "if timegpt_available:\n",
    "    ax1.plot(test_data.index, forecast_timegpt, \n",
    "             label=f'TimeGPT (MAE={timegpt_mae:.2f})', alpha=0.7, linewidth=2, linestyle='--')\n",
    "ax1.plot(test_data.index, forecast_autogluon, \n",
    "         label=f'AutoGluon (MAE={autogluon_mae:.2f})', alpha=0.7, linewidth=2, linestyle='--')\n",
    "ax1.set_title('Forecast Comparison - All Models', fontsize=14, fontweight='bold', pad=15)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Energy Consumption (kWh)', fontsize=12)\n",
    "ax1.legend(loc='upper left', framealpha=0.9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: MAE Comparison\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.9, len(results)))\n",
    "bars = ax2.bar(results['Model'], results['MAE (kWh)'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax2.set_title('Mean Absolute Error', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('MAE (kWh)', fontsize=11)\n",
    "ax2.set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: RMSE Comparison\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "bars = ax3.bar(results['Model'], results['RMSE (kWh)'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax3.set_title('Root Mean Squared Error', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('RMSE (kWh)', fontsize=11)\n",
    "ax3.set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "ax3.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Runtime Comparison\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "bars = ax4.bar(results['Model'], results['Runtime (s)'], color=colors, edgecolor='black', linewidth=1.5)\n",
    "ax4.set_title('Training/Inference Runtime', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Time (seconds)', fontsize=11)\n",
    "ax4.set_xticklabels(results['Model'], rotation=45, ha='right')\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.1f}s', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Time Series Model Comparison Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.savefig('results/model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Visualization saved to: results/model_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Key Insights and Takeaways\n",
    "\n",
    "### Model Performance Summary:\n",
    "\n",
    "1. **Best Overall Accuracy**: AutoGluon achieves the lowest MAE through ensemble learning\n",
    "2. **Foundation Models**: TimesFM and TimeGPT offer excellent accuracy with zero/minimal training\n",
    "3. **Fastest Baseline**: ARIMA provides quick predictions for rapid iteration\n",
    "4. **Deep Learning**: xLSTM balances accuracy with interpretability\n",
    "\n",
    "### When to Use Each Model:\n",
    "\n",
    "| Model | Best For | Trade-off |\n",
    "|-------|----------|----------|\n",
    "| **ARIMA** | Quick baseline, interpretability | Lower accuracy |\n",
    "| **xLSTM** | Complex patterns, some explainability | Training time |\n",
    "| **TimesFM** | Zero-shot learning, no training | Model download |\n",
    "| **TimeGPT** | Best balance, API-based | API dependency |\n",
    "| **AutoGluon** | Production deployment, best accuracy | Training time |\n",
    "\n",
    "### Design Decisions:\n",
    "\n",
    "- **Test Size (30 days)**: Balances evaluation robustness with efficiency\n",
    "- **Sequence Length (30 for LSTM)**: One month of history captures patterns\n",
    "- **Fast Presets**: Used for demonstration; can be tuned for better performance\n",
    "- **Consistent Evaluation**: Same data, metrics, and split for fair comparison\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Feature Engineering**: Add weather, calendar features, energy prices\n",
    "2. **Hyperparameter Tuning**: Optimize for production deployment\n",
    "3. **Cross-Validation**: Time series CV for robust evaluation\n",
    "4. **Ensemble Methods**: Combine top models for better accuracy\n",
    "5. **Production Monitoring**: Track model drift and retrain periodically\n",
    "\n",
    "---\n",
    "\n",
    "**SciPy 2024 Conference | Tacoma, WA**\n",
    "\n",
    "For more details, see: [Full Metrics Report](results/METRICS_SUMMARY.md)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
